{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7e0a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Building the RAG Core Logic and Evaluation\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Add src to path\n",
    "BASE_DIR = Path.cwd().parent\n",
    "sys.path.append(str(BASE_DIR / 'src'))\n",
    "\n",
    "# Import our modules\n",
    "from rag_pipeline import RAGPipeline\n",
    "from evaluator import RAGEvaluator\n",
    "from config import RAGConfig\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db99ef94",
   "metadata": {},
   "source": [
    "### 1. Initialize RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deea170e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Step 1: Initializing RAG Pipeline...\")\n",
    "\n",
    "# Load configuration\n",
    "config = RAGConfig.from_yaml()\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  - Vector Store: {config.vector_store_path}\")\n",
    "print(f\"  - Collection: {config.collection_name}\")\n",
    "print(f\"  - Retriever: top_k={config.retriever.top_k}, threshold={config.retriever.similarity_threshold}\")\n",
    "print(f\"  - Generator: {config.generator.model_name}\")\n",
    "print(f\"  - Temperature: {config.generator.temperature}\")\n",
    "\n",
    "# Initialize pipeline\n",
    "rag_pipeline = RAGPipeline(config=config)\n",
    "\n",
    "# Get pipeline info\n",
    "pipeline_info = rag_pipeline.get_pipeline_info()\n",
    "print(\"\\nPipeline Components:\")\n",
    "print(json.dumps(pipeline_info, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf498a2",
   "metadata": {},
   "source": [
    "### 2. Test Basic Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77d68fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Step 2: Testing Basic Retrieval...\")\n",
    "\n",
    "test_queries = [\n",
    "    \"What are common credit card fee complaints?\",\n",
    "    \"Issues with money transfers\",\n",
    "    \"Problems with savings accounts\",\n",
    "    \"Personal loan application difficulties\"\n",
    "]\n",
    "\n",
    "print(\"\\nTesting retrieval with sample queries:\")\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    \n",
    "    # Analyze query\n",
    "    analysis = rag_pipeline.retriever.analyze_query(query)\n",
    "    print(f\"  Suggested filters: {analysis['suggested_filters']}\")\n",
    "    \n",
    "    # Retrieve\n",
    "    results = rag_pipeline.retriever.retrieve(query, k=3)\n",
    "    \n",
    "    if results:\n",
    "        print(f\"  Retrieved {len(results)} chunks:\")\n",
    "        for i, result in enumerate(results, 1):\n",
    "            product = result['metadata'].get('product_category', 'Unknown')\n",
    "            similarity = result['similarity']\n",
    "            text_preview = result['text'][:100] + \"...\" if len(result['text']) > 100 else result['text']\n",
    "            print(f\"    {i}. [{product}] Similarity: {similarity:.3f}\")\n",
    "            print(f\"       {text_preview}\")\n",
    "    else:\n",
    "        print(\"  No relevant chunks found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c7e541",
   "metadata": {},
   "source": [
    "### 3. Test Full RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd39ff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Step 3: Testing Full RAG Pipeline...\")\n",
    "\n",
    "# Test questions for demonstration\n",
    "demo_questions = [\n",
    "    \"What are customers saying about credit card interest rates?\",\n",
    "    \"How reliable are money transfer services based on complaints?\",\n",
    "    \"What are the main issues with savings accounts?\"\n",
    "]\n",
    "\n",
    "print(\"\\nTesting complete RAG pipeline:\")\n",
    "for i, question in enumerate(demo_questions, 1):\n",
    "    print(f\"\\n{i}. Question: '{question}'\")\n",
    "    \n",
    "    try:\n",
    "        response = rag_pipeline.query(question)\n",
    "        \n",
    "        print(f\"   Answer: {response.answer[:150]}...\")\n",
    "        print(f\"   Retrieved chunks: {response.retrieved_chunks}\")\n",
    "        print(f\"   Processing time: {response.processing_time:.2f}s\")\n",
    "        print(f\"   Model: {response.generation_stats['model']}\")\n",
    "        print(f\"   Tokens: {response.generation_stats['total_tokens']}\")\n",
    "        \n",
    "        # Show sample source\n",
    "        if response.sources:\n",
    "            print(f\"   Sample source: {response.sources[0]['text'][:100]}...\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"   Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb41f97c",
   "metadata": {},
   "source": [
    "### 4. Comprehensive Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bba1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Step 4: Comprehensive Evaluation...\")\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = RAGEvaluator(rag_pipeline)\n",
    "\n",
    "# Load test questions\n",
    "test_questions = evaluator.load_test_questions()\n",
    "print(f\"\\nLoaded {len(test_questions)} test questions\")\n",
    "\n",
    "# Display question categories\n",
    "categories = pd.DataFrame(test_questions)['category'].value_counts()\n",
    "print(\"\\nQuestion Categories:\")\n",
    "for category, count in categories.items():\n",
    "    print(f\"  {category}: {count} questions\")\n",
    "\n",
    "# Run batch evaluation\n",
    "print(\"\\nRunning batch evaluation...\")\n",
    "results_df, summary = evaluator.evaluate_batch(test_questions, save_results=True)\n",
    "\n",
    "# Display evaluation summary\n",
    "print(\"\\nEvaluation Summary:\")\n",
    "print(f\"  Average Overall Score: {summary['avg_overall_score']:.2f}/5\")\n",
    "print(f\"  Average Relevance: {summary['avg_relevance']:.2f}/5\")\n",
    "print(f\"  Average Accuracy: {summary['avg_accuracy']:.2f}/5\")\n",
    "print(f\"  Average Completeness: {summary['avg_completeness']:.2f}/5\")\n",
    "print(f\"  Average Processing Time: {summary['avg_processing_time']:.2f}s\")\n",
    "print(f\"  Total Chunks Retrieved: {summary['total_chunks_retrieved']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ffe309",
   "metadata": {},
   "source": [
    "### 5. Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e691f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Step 5: Visualizing Evaluation Results...\")\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# 1. Score Distribution\n",
    "score_counts = results_df['overall_score'].round().value_counts().sort_index()\n",
    "axes[0, 0].bar(score_counts.index, score_counts.values, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].set_xlabel('Score (1-5)')\n",
    "axes[0, 0].set_ylabel('Number of Questions')\n",
    "axes[0, 0].set_title('Score Distribution')\n",
    "for i, (score, count) in enumerate(score_counts.items()):\n",
    "    axes[0, 0].text(score, count + 0.1, str(count), ha='center')\n",
    "\n",
    "# 2. Performance by Category\n",
    "category_scores = results_df.groupby('category')['overall_score'].mean().sort_values()\n",
    "axes[0, 1].barh(range(len(category_scores)), category_scores.values, color='lightgreen', edgecolor='black')\n",
    "axes[0, 1].set_yticks(range(len(category_scores)))\n",
    "axes[0, 1].set_yticklabels(category_scores.index)\n",
    "axes[0, 1].set_xlabel('Average Score')\n",
    "axes[0, 1].set_title('Performance by Category')\n",
    "for i, score in enumerate(category_scores.values):\n",
    "    axes[0, 1].text(score + 0.05, i, f'{score:.2f}', va='center')\n",
    "\n",
    "# 3. Performance by Difficulty\n",
    "difficulty_scores = results_df.groupby('difficulty')['overall_score'].mean()\n",
    "axes[0, 2].bar(range(len(difficulty_scores)), difficulty_scores.values, \n",
    "               color=['lightcoral', 'gold', 'lightseagreen'], edgecolor='black')\n",
    "axes[0, 2].set_xticks(range(len(difficulty_scores)))\n",
    "axes[0, 2].set_xticklabels(difficulty_scores.index)\n",
    "axes[0, 2].set_ylabel('Average Score')\n",
    "axes[0, 2].set_title('Performance by Difficulty')\n",
    "for i, score in enumerate(difficulty_scores.values):\n",
    "    axes[0, 2].text(i, score + 0.05, f'{score:.2f}', ha='center')\n",
    "\n",
    "# 4. Processing Time Distribution\n",
    "axes[1, 0].hist(results_df['processing_time'], bins=10, color='lightcoral', edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].axvline(results_df['processing_time'].mean(), color='red', linestyle='--', label=f'Mean: {results_df[\"processing_time\"].mean():.2f}s')\n",
    "axes[1, 0].set_xlabel('Processing Time (seconds)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Processing Time Distribution')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Chunks Retrieved vs Score\n",
    "scatter = axes[1, 1].scatter(\n",
    "    results_df['retrieved_sources_count'],\n",
    "    results_df['overall_score'],\n",
    "    c=results_df['processing_time'],\n",
    "    s=100,\n",
    "    alpha=0.6,\n",
    "    cmap='viridis'\n",
    ")\n",
    "axes[1, 1].set_xlabel('Chunks Retrieved')\n",
    "axes[1, 1].set_ylabel('Overall Score')\n",
    "axes[1, 1].set_title('Retrieval vs Score (color=time)')\n",
    "plt.colorbar(scatter, ax=axes[1, 1], label='Processing Time (s)')\n",
    "\n",
    "# 6. Score Components Comparison\n",
    "score_components = ['relevance_score', 'accuracy_score', 'completeness_score']\n",
    "component_means = [results_df[col].mean() for col in score_components]\n",
    "axes[1, 2].bar(range(len(score_components)), component_means, \n",
    "               color=['skyblue', 'lightgreen', 'gold'], edgecolor='black')\n",
    "axes[1, 2].set_xticks(range(len(score_components)))\n",
    "axes[1, 2].set_xticklabels(['Relevance', 'Accuracy', 'Completeness'])\n",
    "axes[1, 2].set_ylabel('Average Score')\n",
    "axes[1, 2].set_title('Score Components Comparison')\n",
    "axes[1, 2].set_ylim([0, 5])\n",
    "for i, mean in enumerate(component_means):\n",
    "    axes[1, 2].text(i, mean + 0.1, f'{mean:.2f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(BASE_DIR / 'evaluation' / 'results' / 'evaluation_visualizations.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329e49e3",
   "metadata": {},
   "source": [
    "### 6. Detailed Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7e055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Step 6: Detailed Analysis of Results...\")\n",
    "\n",
    "# Find best and worst performing questions\n",
    "best_question = results_df.loc[results_df['overall_score'].idxmax()]\n",
    "worst_question = results_df.loc[results_df['overall_score'].idxmin()]\n",
    "\n",
    "print(\"\\nBest Performing Question:\")\n",
    "print(f\"  Question: {best_question['question'][:80]}...\")\n",
    "print(f\"  Category: {best_question['category']}\")\n",
    "print(f\"  Score: {best_question['overall_score']}/5\")\n",
    "print(f\"  Chunks Retrieved: {best_question['retrieved_sources_count']}\")\n",
    "print(f\"  Time: {best_question['processing_time']:.2f}s\")\n",
    "\n",
    "print(\"\\nWorst Performing Question:\")\n",
    "print(f\"  Question: {worst_question['question'][:80]}...\")\n",
    "print(f\"  Category: {worst_question['category']}\")\n",
    "print(f\"  Score: {worst_question['overall_score']}/5\")\n",
    "print(f\"  Chunks Retrieved: {worst_question['retrieved_sources_count']}\")\n",
    "print(f\"  Time: {worst_question['processing_time']:.2f}s\")\n",
    "\n",
    "# Analyze correlation\n",
    "correlation_matrix = results_df[['overall_score', 'relevance_score', 'accuracy_score', \n",
    "                                 'completeness_score', 'processing_time', \n",
    "                                 'retrieved_sources_count']].corr()\n",
    "\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "print(correlation_matrix.round(2))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
