{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4af47784",
   "metadata": {},
   "source": [
    "### 1. Import Libraries and Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f91955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up paths\n",
    "BASE_DIR = Path.cwd().parent\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "RAW_DATA_DIR = DATA_DIR / \"raw\"\n",
    "PROCESSED_DATA_DIR = DATA_DIR / \"processed\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "RAW_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "file_path = RAW_DATA_DIR / \"complaints.csv\"\n",
    "\n",
    "# Load the data (robust to large files)\n",
    "print(\"Loading data...\")\n",
    "# Quick file size check\n",
    "try:\n",
    "    file_size_mb = os.path.getsize(file_path) / (1024**2)\n",
    "except Exception as e:\n",
    "    print('Could not determine file size:', e)\n",
    "\n",
    "# Read a small sample first to inspect columns and dtypes\n",
    "try:\n",
    "    sample = pd.read_csv(file_path, nrows=1000, low_memory=False)\n",
    "except Exception as e:\n",
    "    print('Error reading sample:', e)\n",
    "\n",
    "# Try a normal read but fall back to chunked reading on failure or memory issues\n",
    "try:\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "except Exception as e:\n",
    "    print('Full read failed with error:', type(e).__name__, e)\n",
    "    print('Falling back to chunked read (streaming).')\n",
    "    chunks = []\n",
    "    chunksize = 100000\n",
    "    try:\n",
    "        for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunksize, low_memory=False, iterator=True)):\n",
    "            print(f'  Read chunk {i+1} with {len(chunk):,} rows')\n",
    "            chunks.append(chunk)\n",
    "    except Exception as e2:\n",
    "        print('Chunked read failed:', type(e2).__name__, e2)\n",
    "        raise\n",
    "    df = pd.concat(chunks, ignore_index=True) if chunks else pd.DataFrame()\n",
    "\n",
    "# Display first few rows (or sample if full file not loaded)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(df.head() if not df.empty else (sample.head() if 'sample' in locals() else None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03ecd42",
   "metadata": {},
   "source": [
    "### 2. Initial EDA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c38ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Basic information\n",
    "print(\"=== BASIC INFORMATION ===\")\n",
    "print(f\"Total records: {len(df):,}\")\n",
    "print(f\"Total columns: {len(df.columns)}\")\n",
    "print(\"\\nColumn names and data types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum().sort_values(ascending=False).head(20))\n",
    "\n",
    "# 2. Analyze complaint distribution across products\n",
    "print(\"\\n=== PRODUCT DISTRIBUTION ===\")\n",
    "product_counts = df['Product'].value_counts()\n",
    "print(f\"Number of unique products: {len(product_counts)}\")\n",
    "print(\"\\nTop 20 products by complaint count:\")\n",
    "print(product_counts.head(20))\n",
    "\n",
    "# Visualize product distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_products = product_counts.head(15)\n",
    "bars = plt.barh(top_products.index, top_products.values)\n",
    "plt.xlabel('Number of Complaints')\n",
    "plt.title('Top 15 Products by Complaint Count')\n",
    "plt.gca().invert_yaxis()  # Highest at top\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    plt.text(width + 100, bar.get_y() + bar.get_height()/2, \n",
    "             f'{int(width):,}', ha='left', va='center')\n",
    "plt.tight_layout()\n",
    "plt.savefig(PROCESSED_DATA_DIR / 'product_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 3. Analyze Consumer complaint narrative field\n",
    "print(\"\\n=== CONSUMER COMPLAINT NARRATIVE ANALYSIS ===\")\n",
    "\n",
    "# Check for narratives\n",
    "has_narrative = df['Consumer complaint narrative'].notna()\n",
    "narrative_stats = {\n",
    "    'With Narrative': has_narrative.sum(),\n",
    "    'Without Narrative': (~has_narrative).sum(),\n",
    "    'Percentage with Narrative': (has_narrative.sum() / len(df) * 100)\n",
    "}\n",
    "\n",
    "for key, value in narrative_stats.items():\n",
    "    print(f\"{key}: {value:,.0f}\" if 'Percentage' not in key else f\"{key}: {value:.1f}%\")\n",
    "\n",
    "# Visualize narrative presence\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Pie chart\n",
    "ax[0].pie([narrative_stats['With Narrative'], narrative_stats['Without Narrative']],\n",
    "          labels=['With Narrative', 'Without Narrative'],\n",
    "          autopct='%1.1f%%', startangle=90)\n",
    "ax[0].set_title('Presence of Consumer Complaint Narrative')\n",
    "\n",
    "# Bar chart\n",
    "ax[1].bar(['With Narrative', 'Without Narrative'], \n",
    "          [narrative_stats['With Narrative'], narrative_stats['Without Narrative']])\n",
    "ax[1].set_ylabel('Count')\n",
    "ax[1].set_title('Complaints with/without Narrative')\n",
    "for i, v in enumerate([narrative_stats['With Narrative'], narrative_stats['Without Narrative']]):\n",
    "    ax[1].text(i, v + 1000, f'{v:,}', ha='center', va='bottom')\n",
    "plt.tight_layout()\n",
    "plt.savefig(PROCESSED_DATA_DIR / 'narrative_presence.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 4. Analyze narrative length\n",
    "print(\"\\n=== NARRATIVE LENGTH ANALYSIS ===\")\n",
    "\n",
    "# Calculate word count for narratives that exist\n",
    "df['narrative_word_count'] = df['Consumer complaint narrative'].apply(\n",
    "    lambda x: len(str(x).split()) if pd.notna(x) else 0\n",
    ")\n",
    "\n",
    "# Filter only those with narratives\n",
    "narratives_df = df[df['narrative_word_count'] > 0]\n",
    "\n",
    "print(f\"Number of narratives: {len(narratives_df):,}\")\n",
    "print(f\"Average word count: {narratives_df['narrative_word_count'].mean():.1f}\")\n",
    "print(f\"Median word count: {narratives_df['narrative_word_count'].median():.0f}\")\n",
    "print(f\"Min word count: {narratives_df['narrative_word_count'].min()}\")\n",
    "print(f\"Max word count: {narratives_df['narrative_word_count'].max()}\")\n",
    "\n",
    "# Visualize narrative length distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(narratives_df['narrative_word_count'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Word Count')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of Narrative Length (Word Count)')\n",
    "axes[0].axvline(narratives_df['narrative_word_count'].mean(), color='red', linestyle='--', \n",
    "                label=f'Mean: {narratives_df[\"narrative_word_count\"].mean():.1f}')\n",
    "axes[0].axvline(narratives_df['narrative_word_count'].median(), color='green', linestyle='--', \n",
    "                label=f'Median: {narratives_df[\"narrative_word_count\"].median():.0f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot(narratives_df['narrative_word_count'])\n",
    "axes[1].set_ylabel('Word Count')\n",
    "axes[1].set_title('Box Plot of Narrative Length')\n",
    "axes[1].set_xticklabels([''])\n",
    "\n",
    "# Log scale histogram (to see distribution better)\n",
    "axes[2].hist(narratives_df['narrative_word_count'], bins=50, edgecolor='black', alpha=0.7, log=True)\n",
    "axes[2].set_xlabel('Word Count (log scale)')\n",
    "axes[2].set_ylabel('Frequency (log)')\n",
    "axes[2].set_title('Narrative Length (Log Scale)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PROCESSED_DATA_DIR / 'narrative_length_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 5. Check for very short or very long narratives\n",
    "short_threshold = 10  # Less than 10 words\n",
    "long_threshold = 1000  # More than 1000 words\n",
    "\n",
    "short_narratives = narratives_df[narratives_df['narrative_word_count'] < short_threshold]\n",
    "long_narratives = narratives_df[narratives_df['narrative_word_count'] > long_threshold]\n",
    "\n",
    "print(f\"\\nNarratives with less than {short_threshold} words: {len(short_narratives):,} ({len(short_narratives)/len(narratives_df)*100:.2f}%)\")\n",
    "print(f\"Narratives with more than {long_threshold} words: {len(long_narratives):,} ({len(long_narratives)/len(narratives_df)*100:.2f}%)\")\n",
    "\n",
    "# 6. Analyze by date if available\n",
    "if 'Date received' in df.columns:\n",
    "    print(\"\\n=== TEMPORAL ANALYSIS ===\")\n",
    "    df['Date received'] = pd.to_datetime(df['Date received'], errors='coerce')\n",
    "    df['Year'] = df['Date received'].dt.year\n",
    "    df['Month'] = df['Date received'].dt.month\n",
    "    \n",
    "    yearly_counts = df['Year'].value_counts().sort_index()\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(yearly_counts.index, yearly_counts.values, marker='o')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Number of Complaints')\n",
    "    plt.title('Complaints Over Time')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PROCESSED_DATA_DIR / 'complaints_over_time.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2bf4d1",
   "metadata": {},
   "source": [
    "### Filter and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12646a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Filter to only the four required product categories\n",
    "print(\"=== FILTERING TO REQUIRED PRODUCTS ===\")\n",
    "\n",
    "# Define the products we need (matching CFPB dataset categories)\n",
    "required_products = [\n",
    "    'Credit card',\n",
    "    'Credit card or prepaid card',  # Include variations\n",
    "    'Personal loan',\n",
    "    'Payday loan',  # Sometimes categorized separately\n",
    "    'Student loan',  # Sometimes categorized separately\n",
    "    'Vehicle loan or lease',  # Sometimes categorized separately\n",
    "    'Savings account',\n",
    "    'Checking or savings account',  # Include variations\n",
    "    'Money transfer',\n",
    "    'Virtual currency'  # Sometimes related to money transfers\n",
    "]\n",
    "\n",
    "# Standardize product names (map variations to our 4 main categories)\n",
    "product_mapping = {\n",
    "    # Credit Cards\n",
    "    'Credit card': 'Credit card',\n",
    "    'Credit card or prepaid card': 'Credit card',\n",
    "    'Prepaid card': 'Credit card',\n",
    "    \n",
    "    # Personal Loans\n",
    "    'Personal loan': 'Personal loan',\n",
    "    'Payday loan': 'Personal loan',\n",
    "    'Student loan': 'Personal loan',\n",
    "    'Vehicle loan or lease': 'Personal loan',\n",
    "    'Consumer Loan': 'Personal loan',\n",
    "    \n",
    "    # Savings Accounts\n",
    "    'Savings account': 'Savings account',\n",
    "    'Checking or savings account': 'Savings account',\n",
    "    'Bank account or service': 'Savings account',\n",
    "    \n",
    "    # Money Transfers\n",
    "    'Money transfer': 'Money transfer',\n",
    "    'Virtual currency': 'Money transfer',\n",
    "    'Money transfers': 'Money transfer'\n",
    "}\n",
    "\n",
    "# Create a standardized product column\n",
    "df['Product_standardized'] = df['Product'].map(product_mapping)\n",
    "\n",
    "# Filter to only our 4 main categories\n",
    "filtered_df = df[df['Product_standardized'].isin(['Credit card', 'Personal loan', \n",
    "                                                   'Savings account', 'Money transfer'])]\n",
    "\n",
    "print(f\"Original dataset size: {len(df):,}\")\n",
    "print(f\"Filtered dataset size: {len(filtered_df):,}\")\n",
    "print(f\"Percentage retained: {len(filtered_df)/len(df)*100:.1f}%\")\n",
    "\n",
    "# Show distribution after filtering\n",
    "print(\"\\nDistribution after filtering:\")\n",
    "print(filtered_df['Product_standardized'].value_counts())\n",
    "\n",
    "# 2. Remove records with empty narratives\n",
    "print(\"\\n=== REMOVING EMPTY NARRATIVES ===\")\n",
    "original_count = len(filtered_df)\n",
    "filtered_df = filtered_df[filtered_df['Consumer complaint narrative'].notna() & \n",
    "                          (filtered_df['Consumer complaint narrative'].str.strip() != '')]\n",
    "print(f\"Removed {original_count - len(filtered_df):,} records with empty narratives\")\n",
    "print(f\"Final dataset size: {len(filtered_df):,}\")\n",
    "\n",
    "# 3. Text cleaning function\n",
    "def clean_narrative(text):\n",
    "    \"\"\"\n",
    "    Clean consumer complaint narratives for better embedding quality.\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove common boilerplate phrases\n",
    "    boilerplate_phrases = [\n",
    "        r'i am writing to file a complaint',\n",
    "        r'this is a complaint regarding',\n",
    "        r'to whom it may concern',\n",
    "        r'dear sir/madam',\n",
    "        r'i would like to complain',\n",
    "        r'i am writing to complain',\n",
    "        r'complaint number',\n",
    "        r'case number',\n",
    "        r'reference number',\n",
    "        r'my complaint is about',\n",
    "        r'my complaint concerns',\n",
    "        r'i am writing this complaint',\n",
    "        r'this complaint is about',\n",
    "        r'i am submitting this complaint',\n",
    "        r'please find my complaint below',\n",
    "        r'i am writing to report',\n",
    "        r'i wish to file a complaint',\n",
    "        r'this letter is a formal complaint'\n",
    "    ]\n",
    "    \n",
    "    for phrase in boilerplate_phrases:\n",
    "        text = text.replace(phrase, '')\n",
    "    \n",
    "    # Remove special characters but keep basic punctuation\n",
    "    import re\n",
    "    text = re.sub(r'[^\\w\\s.,!?]', ' ', text)  # Keep basic punctuation\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # Remove leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# 4. Apply cleaning to narratives\n",
    "print(\"\\n=== CLEANING TEXT NARRATIVES ===\")\n",
    "print(\"Applying text cleaning...\")\n",
    "\n",
    "# Create a copy of the narrative column\n",
    "filtered_df['Consumer complaint narrative_original'] = filtered_df['Consumer complaint narrative']\n",
    "filtered_df['Consumer complaint narrative_cleaned'] = filtered_df['Consumer complaint narrative'].apply(clean_narrative)\n",
    "\n",
    "# Check cleaning effect\n",
    "sample_idx = 0\n",
    "print(\"\\nSample cleaning (first complaint):\")\n",
    "print(\"Original:\")\n",
    "print(filtered_df.iloc[sample_idx]['Consumer complaint narrative_original'][:500])\n",
    "print(\"\\nCleaned:\")\n",
    "print(filtered_df.iloc[sample_idx]['Consumer complaint narrative_cleaned'][:500])\n",
    "\n",
    "# 5. Calculate cleaned length statistics\n",
    "filtered_df['cleaned_word_count'] = filtered_df['Consumer complaint narrative_cleaned'].apply(\n",
    "    lambda x: len(str(x).split())\n",
    ")\n",
    "\n",
    "print(\"\\n=== CLEANED NARRATIVE STATISTICS ===\")\n",
    "print(f\"Average word count (cleaned): {filtered_df['cleaned_word_count'].mean():.1f}\")\n",
    "print(f\"Median word count (cleaned): {filtered_df['cleaned_word_count'].median():.0f}\")\n",
    "print(f\"Total words in dataset: {filtered_df['cleaned_word_count'].sum():,}\")\n",
    "\n",
    "# 6. Save the cleaned and filtered dataset\n",
    "print(\"\\n=== SAVING CLEANED DATASET ===\")\n",
    "output_path = PROCESSED_DATA_DIR / \"filtered_complaints.csv\"\n",
    "filtered_df.to_csv(output_path, index=False)\n",
    "print(f\"Saved cleaned dataset to: {output_path}\")\n",
    "print(f\"File size: {output_path.stat().st_size / (1024**2):.2f} MB\")\n",
    "\n",
    "# 7. Summary statistics for report\n",
    "print(\"\\n=== FINAL SUMMARY ===\")\n",
    "print(f\"Total complaints in final dataset: {len(filtered_df):,}\")\n",
    "print(\"\\nProduct distribution:\")\n",
    "for product, count in filtered_df['Product_standardized'].value_counts().items():\n",
    "    percentage = count / len(filtered_df) * 100\n",
    "    print(f\"  - {product}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nAverage narrative length: {filtered_df['cleaned_word_count'].mean():.1f} words\")\n",
    "print(f\"Median narrative length: {filtered_df['cleaned_word_count'].median():.0f} words\")\n",
    "print(f\"Shortest narrative: {filtered_df['cleaned_word_count'].min()} words\")\n",
    "print(f\"Longest narrative: {filtered_df['cleaned_word_count'].max()} words\")\n",
    "\n",
    "# 8. Visualize final distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Product distribution\n",
    "product_counts = filtered_df['Product_standardized'].value_counts()\n",
    "axes[0].bar(product_counts.index, product_counts.values, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
    "axes[0].set_xlabel('Product Category')\n",
    "axes[0].set_ylabel('Number of Complaints')\n",
    "axes[0].set_title('Complaint Distribution by Product Category')\n",
    "for i, v in enumerate(product_counts.values):\n",
    "    axes[0].text(i, v + 100, f'{v:,}', ha='center', va='bottom')\n",
    "\n",
    "# Narrative length by product\n",
    "product_groups = []\n",
    "for product in filtered_df['Product_standardized'].unique():\n",
    "    product_data = filtered_df[filtered_df['Product_standardized'] == product]\n",
    "    product_groups.append(product_data['cleaned_word_count'].values)\n",
    "\n",
    "axes[1].boxplot(product_groups, labels=product_counts.index)\n",
    "axes[1].set_xlabel('Product Category')\n",
    "axes[1].set_ylabel('Word Count')\n",
    "axes[1].set_title('Narrative Length Distribution by Product Category')\n",
    "axes[1].set_yscale('log')  # Use log scale for better visualization\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PROCESSED_DATA_DIR / 'final_distribution_summary.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
